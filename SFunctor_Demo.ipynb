{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFunctor Demo: Structure Function Analysis for MHD Turbulence\n",
    "\n",
    "This notebook provides a comprehensive demonstration of the SFunctor pipeline for analyzing anisotropic, angle-resolved structure functions from 2D slices of 3D magnetohydrodynamic (MHD) simulations.\n",
    "\n",
    "## Overview\n",
    "\n",
    "SFunctor is designed to:\n",
    "- Extract 2D slices from 3D MHD simulation data\n",
    "- Compute various physics quantities (velocity, magnetic field, density, Elsasser variables)\n",
    "- Calculate structure functions with different displacement vectors\n",
    "- Visualize turbulence statistics and scaling behaviors\n",
    "\n",
    "This demo will walk you through:\n",
    "1. Loading and exploring MHD simulation slices\n",
    "2. Running structure function analysis (non-MPI version)\n",
    "3. Creating comprehensive visualizations\n",
    "4. Understanding the physics and computational pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, SymLogNorm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Interactive widgets (optional, will use if available)\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    HAS_WIDGETS = True\n",
    "except ImportError:\n",
    "    HAS_WIDGETS = False\n",
    "    print(\"ipywidgets not available. Interactive features will be limited.\")\n",
    "\n",
    "# SFunctor modules\n",
    "from sf_io import load_slice_npz, parse_slice_metadata\n",
    "from sf_physics import compute_vA, compute_z_plus_minus\n",
    "from sf_displacements import find_ell_bin_edges, build_displacement_list\n",
    "\n",
    "# Set matplotlib parameters for better plots\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what data files are available\n",
    "data_dir = Path(\"slice_data\")\n",
    "if not data_dir.exists():\n",
    "    print(f\"Error: {data_dir} directory not found!\")\n",
    "    print(\"Please make sure you're running this notebook from the SFunctor root directory.\")\n",
    "else:\n",
    "    # List available slice files\n",
    "    slice_files = list(data_dir.glob(\"slice_*.npz\"))\n",
    "    slice_files = [f for f in slice_files if not f.stem.startswith(\"simple_sf_\")]\n",
    "    \n",
    "    print(f\"Found {len(slice_files)} slice files:\")\n",
    "    for f in sorted(slice_files):\n",
    "        print(f\"  - {f.name}\")\n",
    "    \n",
    "    # List available structure function results\n",
    "    sf_files = list(data_dir.glob(\"simple_sf_*.npz\"))\n",
    "    if sf_files:\n",
    "        print(f\"\\nFound {len(sf_files)} pre-computed structure function results:\")\n",
    "        for f in sorted(sf_files):\n",
    "            print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a slice list file for batch processing\n",
    "slice_list_file = data_dir / \"my_slices.txt\"\n",
    "\n",
    "# Gather all available slices\n",
    "all_slice_files = list(data_dir.glob(\"slice_*.npz\"))\n",
    "all_slice_files = [f for f in all_slice_files if not f.stem.startswith(\"simple_sf_\")]\n",
    "\n",
    "if all_slice_files:\n",
    "    # Write slice paths to a list file\n",
    "    with open(slice_list_file, 'w') as f:\n",
    "        for slice_path in sorted(all_slice_files):\n",
    "            f.write(str(slice_path) + '\\n')\n",
    "    \n",
    "    print(f\"Created slice list file: {slice_list_file}\")\n",
    "    print(f\"Contains {len(all_slice_files)} slices\")\n",
    "    \n",
    "    # Show example of how to process all slices\n",
    "    print(\"\\nTo process all slices with structure function analysis:\")\n",
    "    print(f\"python run_sf.py --slice_list {slice_list_file} --stride 2\")\n",
    "    \n",
    "    # Or for MPI parallel processing:\n",
    "    print(\"\\nFor parallel processing with MPI:\")\n",
    "    print(f\"mpirun -n 8 python run_sf.py --slice_list {slice_list_file} --stride 2\")\n",
    "else:\n",
    "    print(\"No slice files available for batch processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing Multiple Slices\n",
    "\n",
    "For systematic analysis, you often want to extract many slices. Here's how to create a batch of slices and prepare them for structure function analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Direct Python extraction (if you prefer not to use subprocess)\n",
    "# This shows how to extract a single slice programmatically\n",
    "\n",
    "if raw_data_dir.exists() and sim_dirs:\n",
    "    # Import the extraction function\n",
    "    try:\n",
    "        from extract_2d_slice import extract_2d_slice\n",
    "        \n",
    "        sim_name = sim_dirs[0].name.replace(\"data_\", \"\")\n",
    "        \n",
    "        print(\"Extracting a single slice using Python function directly...\")\n",
    "        \n",
    "        # Extract one slice as an example\n",
    "        slice_data_direct = extract_2d_slice(\n",
    "            sim_name=sim_name,\n",
    "            axis=3,            # z-normal slice\n",
    "            slice_value=0.0,   # at the midplane\n",
    "            file_number=0,     # first snapshot\n",
    "            save=True,         # save to cache\n",
    "            cache_dir=\"slice_data\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nExtracted slice with shape: {slice_data_direct['dens'].shape}\")\n",
    "        print(f\"Available fields: {list(slice_data_direct.keys())}\")\n",
    "        \n",
    "        # Quick visualization of the extracted density\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        im = ax.imshow(slice_data_direct['dens'], origin='lower', cmap='plasma')\n",
    "        ax.set_title(f'Density Field - Direct Extraction\\n{sim_name} (z=0 slice)')\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        plt.colorbar(im, ax=ax, label='Density')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"Could not import extract_2d_slice: {e}\")\n",
    "        print(\"The extraction module may have dependencies that are not available.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during direct extraction: {e}\")\n",
    "        print(\"This may be due to the specific format of your simulation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Slice Extraction Parameters\n",
    "\n",
    "The slice extraction tool has several important parameters:\n",
    "\n",
    "- **sim_name**: The simulation directory name (without the `data_` prefix)\n",
    "- **axes**: Which axes to slice along (1=x, 2=y, 3=z)\n",
    "- **offsets**: Positions along each axis as fractions of box size (-0.5 to 0.5)\n",
    "- **file_number**: Which snapshot to use (0 = first, None = latest)\n",
    "- **plot**: Whether to generate PNG visualizations of the density field\n",
    "\n",
    "The extracted slices contain:\n",
    "- Primary fields: `dens`, `velx`, `vely`, `velz`, `bcc1`, `bcc2`, `bcc3`\n",
    "- Derived fields: `vortx`, `vorty`, `vortz` (vorticity), `currx`, `curry`, `currz` (current density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract slices using subprocess (as done in demo_pipeline.py)\n",
    "import subprocess\n",
    "\n",
    "# Only run if we have simulation data\n",
    "if raw_data_dir.exists() and sim_dirs:\n",
    "    # Use the first available simulation\n",
    "    sim_name = sim_dirs[0].name.replace(\"data_\", \"\")\n",
    "    \n",
    "    print(f\"Extracting slices from simulation: {sim_name}\")\n",
    "    print(\"\\nThis will extract:\")\n",
    "    print(\"  - 2 slice orientations (x-normal and z-normal)\")\n",
    "    print(\"  - 3 positions along each axis (-0.25, 0.0, 0.25)\")\n",
    "    print(\"  - Total: 6 slices\\n\")\n",
    "    \n",
    "    # Build the command\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"run_extract_slice.py\",\n",
    "        \"--sim_name\", sim_name,\n",
    "        \"--offsets\", \"-0.25,0.0,0.25\",   # Three positions\n",
    "        \"--axes\", \"1,3\",                  # x and z normal slices\n",
    "        \"--file_number\", \"0\",             # Use first snapshot\n",
    "        \"--plot\",                         # Generate density plots\n",
    "    ]\n",
    "    \n",
    "    print(\"Running command:\")\n",
    "    print(\" \".join(cmd))\n",
    "    print(\"\\nThis may take a minute...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Run the extraction\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        print(\"Extraction successful!\")\n",
    "        print(\"\\nOutput:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Check what was created\n",
    "        new_slices = list(data_dir.glob(f\"slice_*_{sim_name}_*.npz\"))\n",
    "        if new_slices:\n",
    "            print(f\"\\nCreated {len(new_slices)} new slice files:\")\n",
    "            for f in sorted(new_slices)[:6]:\n",
    "                print(f\"  - {f.name}\")\n",
    "                \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during extraction: {e}\")\n",
    "        print(f\"Error output: {e.stderr}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping slice extraction demo (no simulation data found)\")\n",
    "    print(\"\\nTo extract slices from your own data:\")\n",
    "    print(\"1. Place your 3D simulation data in data/data_YourSimName/\")\n",
    "    print(\"2. Run: python run_extract_slice.py --sim_name YourSimName --axes 1,2,3 --offsets -0.25,0,0.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using run_extract_slice.py\n",
    "\n",
    "The primary tool for extracting slices is `run_extract_slice.py`. It can extract multiple slices at different positions and orientations in a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available raw data\n",
    "raw_data_dir = Path(\"data\")\n",
    "if raw_data_dir.exists():\n",
    "    print(\"Available simulation data:\")\n",
    "    sim_dirs = list(raw_data_dir.glob(\"data_*\"))\n",
    "    for sim_dir in sim_dirs:\n",
    "        sim_name = sim_dir.name.replace(\"data_\", \"\")\n",
    "        print(f\"\\n  Simulation: {sim_name}\")\n",
    "        \n",
    "        # Check for binary files\n",
    "        bin_files = list(sim_dir.glob(\"bin/rank_*/Turb.*.bin\"))\n",
    "        if bin_files:\n",
    "            print(f\"    - Found {len(bin_files)} binary data files\")\n",
    "            # Show a few example files\n",
    "            for f in sorted(bin_files)[:3]:\n",
    "                print(f\"      {f.relative_to(raw_data_dir)}\")\n",
    "            if len(bin_files) > 3:\n",
    "                print(f\"      ... and {len(bin_files) - 3} more files\")\n",
    "else:\n",
    "    print(\"No raw data directory found.\")\n",
    "    print(\"To use slice extraction, place your 3D simulation data in a 'data/' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Extract 2D Slices from 3D Data\n",
    "\n",
    "Before analyzing structure functions, we need to extract 2D slices from 3D simulation data. SFunctor provides tools to extract slices at various positions and orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore a 2D Slice\n",
    "\n",
    "Let's load one of the example slices and explore its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a slice file to analyze\n",
    "if slice_files:\n",
    "    slice_file = slice_files[0]  # Use the first available slice\n",
    "    print(f\"Loading slice: {slice_file}\")\n",
    "    \n",
    "    # Load the slice data\n",
    "    slice_data = load_slice_npz(slice_file, stride=1)  # stride=1 for full resolution\n",
    "    \n",
    "    # Parse metadata from filename\n",
    "    axis, beta = parse_slice_metadata(slice_file)\n",
    "    \n",
    "    # Extract fields\n",
    "    rho = slice_data[\"rho\"]\n",
    "    v_x = slice_data[\"v_x\"]\n",
    "    v_y = slice_data[\"v_y\"]\n",
    "    v_z = slice_data[\"v_z\"]\n",
    "    B_x = slice_data[\"B_x\"]\n",
    "    B_y = slice_data[\"B_y\"]\n",
    "    B_z = slice_data[\"B_z\"]\n",
    "    \n",
    "    print(f\"\\nSlice properties:\")\n",
    "    print(f\"  - Shape: {rho.shape}\")\n",
    "    print(f\"  - Slice axis: {axis}\")\n",
    "    print(f\"  - Plasma beta: {beta}\")\n",
    "    print(f\"\\nField statistics:\")\n",
    "    print(f\"  - Density: min={rho.min():.3f}, max={rho.max():.3f}, mean={rho.mean():.3f}\")\n",
    "    print(f\"  - |v|: min={np.sqrt(v_x**2 + v_y**2 + v_z**2).min():.3f}, max={np.sqrt(v_x**2 + v_y**2 + v_z**2).max():.3f}\")\n",
    "    print(f\"  - |B|: min={np.sqrt(B_x**2 + B_y**2 + B_z**2).min():.3f}, max={np.sqrt(B_x**2 + B_y**2 + B_z**2).max():.3f}\")\n",
    "else:\n",
    "    print(\"No slice files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the Slice Fields\n",
    "\n",
    "Let's create visualizations of the primary fields in the slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive visualization of the slice\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "gs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Helper function for field plots\n",
    "def plot_field(ax, field, title, cmap='viridis', norm=None):\n",
    "    im = ax.imshow(field, origin='lower', cmap=cmap, norm=norm)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    return im\n",
    "\n",
    "# Density\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "plot_field(ax1, rho, 'Density ρ', cmap='plasma')\n",
    "\n",
    "# Velocity magnitude\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "v_mag = np.sqrt(v_x**2 + v_y**2 + v_z**2)\n",
    "plot_field(ax2, v_mag, 'Velocity Magnitude |v|', cmap='viridis')\n",
    "\n",
    "# Magnetic field magnitude\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "B_mag = np.sqrt(B_x**2 + B_y**2 + B_z**2)\n",
    "plot_field(ax3, B_mag, 'Magnetic Field Magnitude |B|', cmap='inferno')\n",
    "\n",
    "# Velocity components\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "plot_field(ax4, v_x, 'Velocity vₓ', cmap='RdBu_r', norm=SymLogNorm(linthresh=0.01))\n",
    "\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "plot_field(ax5, v_y, 'Velocity vᵧ', cmap='RdBu_r', norm=SymLogNorm(linthresh=0.01))\n",
    "\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "plot_field(ax6, v_z, 'Velocity vᵤ', cmap='RdBu_r', norm=SymLogNorm(linthresh=0.01))\n",
    "\n",
    "# Compute and plot derived quantities\n",
    "# Alfvén velocity\n",
    "vA_x, vA_y, vA_z = compute_vA(B_x, B_y, B_z, rho)\n",
    "vA_mag = np.sqrt(vA_x**2 + vA_y**2 + vA_z**2)\n",
    "\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "plot_field(ax7, vA_mag, 'Alfvén Velocity |vₐ|', cmap='copper')\n",
    "\n",
    "# Elsasser variables\n",
    "(zp_x, zp_y, zp_z), (zm_x, zm_y, zm_z) = compute_z_plus_minus(v_x, v_y, v_z, vA_x, vA_y, vA_z)\n",
    "zp_mag = np.sqrt(zp_x**2 + zp_y**2 + zp_z**2)\n",
    "zm_mag = np.sqrt(zm_x**2 + zm_y**2 + zm_z**2)\n",
    "\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "plot_field(ax8, zp_mag, 'Elsasser z⁺ = v + vₐ', cmap='spring')\n",
    "\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "plot_field(ax9, zm_mag, 'Elsasser z⁻ = v - vₐ', cmap='autumn')\n",
    "\n",
    "plt.suptitle(f'MHD Slice Visualization: {slice_file.name}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Structure Function Analysis\n",
    "\n",
    "Now let's perform a simplified structure function analysis. This version doesn't use MPI or Numba, making it easier to understand and modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simplified structure function computation\n",
    "def compute_structure_functions_simple(\n",
    "    v_x, v_y, v_z, B_x, B_y, B_z, rho,\n",
    "    delta_i, delta_j, N_random_subsamples=100\n",
    "):\n",
    "    \"\"\"Simplified structure function computation.\"\"\"\n",
    "    N, M = v_x.shape\n",
    "    \n",
    "    # Random spatial samples\n",
    "    flat_indices = np.random.choice(M * N, size=N_random_subsamples, replace=False)\n",
    "    random_points_y = flat_indices // M\n",
    "    random_points_x = flat_indices % M\n",
    "    \n",
    "    # Storage for structure function values\n",
    "    dv_values = []\n",
    "    dB_values = []\n",
    "    drho_values = []\n",
    "    \n",
    "    # Also store individual components for more detailed analysis\n",
    "    dv_components = {'x': [], 'y': [], 'z': []}\n",
    "    dB_components = {'x': [], 'y': [], 'z': []}\n",
    "    \n",
    "    for idx in range(N_random_subsamples):\n",
    "        i = random_points_x[idx]\n",
    "        j = random_points_y[idx]\n",
    "        ip = (i + delta_i) % M\n",
    "        jp = (j + delta_j) % N\n",
    "        \n",
    "        # Compute differences using 2-point stencil\n",
    "        dvx = v_x[jp, ip] - v_x[j, i]\n",
    "        dvy = v_y[jp, ip] - v_y[j, i]\n",
    "        dvz = v_z[jp, ip] - v_z[j, i]\n",
    "        \n",
    "        dBx = B_x[jp, ip] - B_x[j, i]\n",
    "        dBy = B_y[jp, ip] - B_y[j, i]\n",
    "        dBz = B_z[jp, ip] - B_z[j, i]\n",
    "        \n",
    "        drho = rho[jp, ip] - rho[j, i]\n",
    "        \n",
    "        # Store components\n",
    "        dv_components['x'].append(dvx)\n",
    "        dv_components['y'].append(dvy)\n",
    "        dv_components['z'].append(dvz)\n",
    "        dB_components['x'].append(dBx)\n",
    "        dB_components['y'].append(dBy)\n",
    "        dB_components['z'].append(dBz)\n",
    "        \n",
    "        # Compute magnitudes\n",
    "        dv = np.sqrt(dvx**2 + dvy**2 + dvz**2)\n",
    "        dB = np.sqrt(dBx**2 + dBy**2 + dBz**2)\n",
    "        drho_abs = abs(drho)\n",
    "        \n",
    "        dv_values.append(dv)\n",
    "        dB_values.append(dB)\n",
    "        drho_values.append(drho_abs)\n",
    "    \n",
    "    return (np.array(dv_values), np.array(dB_values), np.array(drho_values),\n",
    "            {k: np.array(v) for k, v in dv_components.items()},\n",
    "            {k: np.array(v) for k, v in dB_components.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up displacement vectors\n",
    "N_res = rho.shape[0]\n",
    "ell_max = N_res // 4  # Conservative choice to avoid boundary effects\n",
    "n_ell_bins = 10  # Number of logarithmic bins for displacement magnitude\n",
    "n_disp_total = 50  # Total number of displacement vectors to sample\n",
    "\n",
    "# Generate displacement bins\n",
    "ell_bin_edges = find_ell_bin_edges(1.0, ell_max, n_ell_bins)\n",
    "displacements = build_displacement_list(ell_bin_edges, n_disp_total)\n",
    "\n",
    "print(f\"Grid size: {N_res}x{N_res}\")\n",
    "print(f\"Maximum displacement: {ell_max} pixels\")\n",
    "print(f\"Number of displacement bins: {n_ell_bins}\")\n",
    "print(f\"Total displacement vectors: {len(displacements)}\")\n",
    "print(f\"\\nFirst few displacements:\")\n",
    "for i, (dx, dy) in enumerate(displacements[:5]):\n",
    "    r = np.sqrt(dx**2 + dy**2)\n",
    "    print(f\"  {i}: (Δx={dx:6.2f}, Δy={dy:6.2f}) → r={r:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the structure function analysis\n",
    "print(\"Computing structure functions...\")\n",
    "print(\"(This may take a minute for large grids)\\n\")\n",
    "\n",
    "# Storage for all results\n",
    "all_displacements = []\n",
    "all_dv_values = []\n",
    "all_dB_values = []\n",
    "all_drho_values = []\n",
    "all_dv_components = {'x': [], 'y': [], 'z': []}\n",
    "all_dB_components = {'x': [], 'y': [], 'z': []}\n",
    "\n",
    "# Process a subset of displacements for demonstration\n",
    "n_disp_demo = min(20, len(displacements))  # Limit for faster execution\n",
    "N_random_subsamples = 200  # Number of random points per displacement\n",
    "\n",
    "for disp_idx, (dx, dy) in enumerate(displacements[:n_disp_demo]):\n",
    "    if disp_idx % 5 == 0:\n",
    "        print(f\"Processing displacement {disp_idx+1}/{n_disp_demo}...\")\n",
    "    \n",
    "    # Compute structure functions for this displacement\n",
    "    dv_vals, dB_vals, drho_vals, dv_comp, dB_comp = compute_structure_functions_simple(\n",
    "        v_x, v_y, v_z, B_x, B_y, B_z, rho,\n",
    "        int(dx), int(dy), N_random_subsamples\n",
    "    )\n",
    "    \n",
    "    # Store displacement magnitude\n",
    "    r = np.sqrt(dx**2 + dy**2)\n",
    "    all_displacements.extend([r] * len(dv_vals))\n",
    "    all_dv_values.extend(dv_vals)\n",
    "    all_dB_values.extend(dB_vals)\n",
    "    all_drho_values.extend(drho_vals)\n",
    "    \n",
    "    # Store components\n",
    "    for comp in ['x', 'y', 'z']:\n",
    "        all_dv_components[comp].extend(dv_comp[comp])\n",
    "        all_dB_components[comp].extend(dB_comp[comp])\n",
    "\n",
    "# Convert to arrays\n",
    "all_displacements = np.array(all_displacements)\n",
    "all_dv_values = np.array(all_dv_values)\n",
    "all_dB_values = np.array(all_dB_values)\n",
    "all_drho_values = np.array(all_drho_values)\n",
    "for comp in ['x', 'y', 'z']:\n",
    "    all_dv_components[comp] = np.array(all_dv_components[comp])\n",
    "    all_dB_components[comp] = np.array(all_dB_components[comp])\n",
    "\n",
    "print(f\"\\nAnalysis complete!\")\n",
    "print(f\"Total samples: {len(all_dv_values)}\")\n",
    "print(f\"Velocity SF range: {all_dv_values.min():.6f} - {all_dv_values.max():.6f}\")\n",
    "print(f\"Magnetic SF range: {all_dB_values.min():.6f} - {all_dB_values.max():.6f}\")\n",
    "print(f\"Density SF range: {all_drho_values.min():.6f} - {all_drho_values.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Structure Function Visualization\n",
    "\n",
    "Now let's create various visualizations to analyze the structure functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the data by displacement\n",
    "r_bins = np.logspace(np.log10(all_displacements.min()), \n",
    "                     np.log10(all_displacements.max()), 15)\n",
    "r_centers = 0.5 * (r_bins[1:] + r_bins[:-1])\n",
    "\n",
    "# Compute binned statistics\n",
    "def bin_statistics(displacements, values, bins):\n",
    "    \"\"\"Compute mean and std in each bin.\"\"\"\n",
    "    means = []\n",
    "    stds = []\n",
    "    counts = []\n",
    "    \n",
    "    for i in range(len(bins) - 1):\n",
    "        mask = (displacements >= bins[i]) & (displacements < bins[i+1])\n",
    "        if np.sum(mask) > 0:\n",
    "            means.append(np.mean(values[mask]))\n",
    "            stds.append(np.std(values[mask]))\n",
    "            counts.append(np.sum(mask))\n",
    "        else:\n",
    "            means.append(np.nan)\n",
    "            stds.append(np.nan)\n",
    "            counts.append(0)\n",
    "    \n",
    "    return np.array(means), np.array(stds), np.array(counts)\n",
    "\n",
    "# Compute binned statistics for each field\n",
    "dv_mean, dv_std, dv_counts = bin_statistics(all_displacements, all_dv_values, r_bins)\n",
    "dB_mean, dB_std, dB_counts = bin_statistics(all_displacements, all_dB_values, r_bins)\n",
    "drho_mean, drho_std, drho_counts = bin_statistics(all_displacements, all_drho_values, r_bins)\n",
    "\n",
    "# Remove bins with no data\n",
    "valid = ~np.isnan(dv_mean)\n",
    "r_centers_valid = r_centers[valid]\n",
    "dv_mean_valid = dv_mean[valid]\n",
    "dB_mean_valid = dB_mean[valid]\n",
    "drho_mean_valid = drho_mean[valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Structure functions vs displacement (with theoretical scalings)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.loglog(r_centers_valid, dv_mean_valid, 'o-', label='Velocity |δv|', color='blue', markersize=8)\n",
    "ax1.loglog(r_centers_valid, dB_mean_valid, 's-', label='Magnetic |δB|', color='red', markersize=8)\n",
    "ax1.loglog(r_centers_valid, drho_mean_valid, '^-', label='Density |δρ|', color='green', markersize=8)\n",
    "\n",
    "# Add theoretical scaling lines\n",
    "r_theory = np.linspace(r_centers_valid.min(), r_centers_valid.max(), 100)\n",
    "\n",
    "# Kolmogorov scaling for velocity (1/3)\n",
    "idx_mid = len(r_centers_valid)//2\n",
    "C_v = dv_mean_valid[idx_mid] / (r_centers_valid[idx_mid]**(1/3))\n",
    "ax1.loglog(r_theory, C_v * r_theory**(1/3), '--', alpha=0.5, color='blue', label='r^(1/3)')\n",
    "\n",
    "# Iroshnikov-Kraichnan scaling (1/2)\n",
    "C_B = dB_mean_valid[idx_mid] / (r_centers_valid[idx_mid]**(1/2))\n",
    "ax1.loglog(r_theory, C_B * r_theory**(1/2), '--', alpha=0.5, color='red', label='r^(1/2)')\n",
    "\n",
    "# Linear scaling\n",
    "C_rho = drho_mean_valid[idx_mid] / r_centers_valid[idx_mid]\n",
    "ax1.loglog(r_theory, C_rho * r_theory, '--', alpha=0.5, color='green', label='r^1')\n",
    "\n",
    "ax1.set_xlabel('Displacement r [pixels]', fontsize=12)\n",
    "ax1.set_ylabel('Structure Function', fontsize=12)\n",
    "ax1.set_title('Structure Functions with Theoretical Scalings', fontsize=14)\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Probability distributions\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.hist(all_dv_values, bins=50, alpha=0.7, density=True, color='blue', edgecolor='black')\n",
    "ax2.set_xlabel('|δv|')\n",
    "ax2.set_ylabel('PDF')\n",
    "ax2.set_title('Velocity Increment Distribution')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.hist(all_dB_values, bins=50, alpha=0.7, density=True, color='red', edgecolor='black')\n",
    "ax3.set_xlabel('|δB|')\n",
    "ax3.set_ylabel('PDF')\n",
    "ax3.set_title('Magnetic Increment Distribution')\n",
    "ax3.set_yscale('log')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "ax4.hist(all_drho_values, bins=50, alpha=0.7, density=True, color='green', edgecolor='black')\n",
    "ax4.set_xlabel('|δρ|')\n",
    "ax4.set_ylabel('PDF')\n",
    "ax4.set_title('Density Increment Distribution')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Component analysis\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "components = ['x', 'y', 'z']\n",
    "colors = ['red', 'green', 'blue']\n",
    "for comp, color in zip(components, colors):\n",
    "    ax5.hist(all_dv_components[comp], bins=50, alpha=0.5, density=True, \n",
    "             label=f'δv_{comp}', color=color, edgecolor='black')\n",
    "ax5.set_xlabel('δv components')\n",
    "ax5.set_ylabel('PDF')\n",
    "ax5.set_title('Velocity Component Distributions')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scatter plot of velocity vs magnetic increments\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "# Sample points for clarity\n",
    "sample_idx = np.random.choice(len(all_dv_values), size=min(1000, len(all_dv_values)), replace=False)\n",
    "ax6.scatter(all_dv_values[sample_idx], all_dB_values[sample_idx], \n",
    "            alpha=0.5, s=20, c=all_displacements[sample_idx], cmap='viridis')\n",
    "cbar = plt.colorbar(ax6.scatter([], [], c=[], cmap='viridis'), ax=ax6)\n",
    "cbar.set_label('Displacement r')\n",
    "ax6.set_xlabel('|δv|')\n",
    "ax6.set_ylabel('|δB|')\n",
    "ax6.set_title('Velocity vs Magnetic Increments')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Local slopes (scaling exponents)\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "if len(r_centers_valid) > 3:\n",
    "    # Compute local slopes\n",
    "    def local_slope(x, y):\n",
    "        \"\"\"Compute local slope in log-log space.\"\"\"\n",
    "        log_x = np.log10(x)\n",
    "        log_y = np.log10(y)\n",
    "        slopes = np.diff(log_y) / np.diff(log_x)\n",
    "        x_mid = 0.5 * (x[1:] + x[:-1])\n",
    "        return x_mid, slopes\n",
    "    \n",
    "    r_mid_v, slopes_v = local_slope(r_centers_valid, dv_mean_valid)\n",
    "    r_mid_B, slopes_B = local_slope(r_centers_valid, dB_mean_valid)\n",
    "    r_mid_rho, slopes_rho = local_slope(r_centers_valid, drho_mean_valid)\n",
    "    \n",
    "    ax7.semilogx(r_mid_v, slopes_v, 'o-', label='Velocity', color='blue')\n",
    "    ax7.semilogx(r_mid_B, slopes_B, 's-', label='Magnetic', color='red')\n",
    "    ax7.semilogx(r_mid_rho, slopes_rho, '^-', label='Density', color='green')\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax7.axhline(1/3, linestyle='--', alpha=0.5, color='blue', label='1/3')\n",
    "    ax7.axhline(1/2, linestyle='--', alpha=0.5, color='red', label='1/2')\n",
    "    ax7.axhline(1, linestyle='--', alpha=0.5, color='green', label='1')\n",
    "    \n",
    "    ax7.set_xlabel('Displacement r [pixels]')\n",
    "    ax7.set_ylabel('Local Scaling Exponent')\n",
    "    ax7.set_title('Scale-Dependent Exponents')\n",
    "    ax7.legend(loc='best', fontsize=9)\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    ax7.set_ylim(0, 1.2)\n",
    "\n",
    "plt.suptitle(f'Structure Function Analysis: {slice_file.name}', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Elsasser Variable Structure Functions\n",
    "\n",
    "Let's also compute structure functions for the Elsasser variables, which are important for understanding MHD turbulence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute structure functions for Elsasser variables\n",
    "print(\"Computing Elsasser variable structure functions...\")\n",
    "\n",
    "# Compute Alfvén velocity and Elsasser variables\n",
    "vA_x, vA_y, vA_z = compute_vA(B_x, B_y, B_z, rho)\n",
    "(zp_x, zp_y, zp_z), (zm_x, zm_y, zm_z) = compute_z_plus_minus(v_x, v_y, v_z, vA_x, vA_y, vA_z)\n",
    "\n",
    "# Storage for Elsasser structure functions\n",
    "all_dzp_values = []\n",
    "all_dzm_values = []\n",
    "\n",
    "# Compute for a subset of displacements\n",
    "for disp_idx, (dx, dy) in enumerate(displacements[:n_disp_demo]):\n",
    "    if disp_idx % 5 == 0:\n",
    "        print(f\"Processing displacement {disp_idx+1}/{n_disp_demo}...\")\n",
    "    \n",
    "    # Use the same function but with Elsasser variables\n",
    "    dzp_vals, dzm_vals, _, _, _ = compute_structure_functions_simple(\n",
    "        zp_x, zp_y, zp_z, zm_x, zm_y, zm_z, rho,  # Note: using z- in place of B for simplicity\n",
    "        int(dx), int(dy), N_random_subsamples\n",
    "    )\n",
    "    \n",
    "    all_dzp_values.extend(dzp_vals)\n",
    "    all_dzm_values.extend(dzm_vals)\n",
    "\n",
    "all_dzp_values = np.array(all_dzp_values)\n",
    "all_dzm_values = np.array(all_dzm_values)\n",
    "\n",
    "print(f\"\\nElsasser z⁺ SF range: {all_dzp_values.min():.6f} - {all_dzp_values.max():.6f}\")\n",
    "print(f\"Elsasser z⁻ SF range: {all_dzm_values.min():.6f} - {all_dzm_values.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin Elsasser structure functions\n",
    "dzp_mean, dzp_std, _ = bin_statistics(all_displacements, all_dzp_values, r_bins)\n",
    "dzm_mean, dzm_std, _ = bin_statistics(all_displacements, all_dzm_values, r_bins)\n",
    "\n",
    "# Plot comparison of all structure functions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# All structure functions together\n",
    "ax1.loglog(r_centers_valid, dv_mean_valid, 'o-', label='|δv|', color='blue', markersize=8)\n",
    "ax1.loglog(r_centers_valid, dB_mean_valid, 's-', label='|δB|', color='red', markersize=8)\n",
    "ax1.loglog(r_centers_valid, drho_mean_valid, '^-', label='|δρ|', color='green', markersize=8)\n",
    "ax1.loglog(r_centers[valid], dzp_mean[valid], 'd-', label='|δz⁺|', color='purple', markersize=8)\n",
    "ax1.loglog(r_centers[valid], dzm_mean[valid], 'v-', label='|δz⁻|', color='orange', markersize=8)\n",
    "\n",
    "ax1.set_xlabel('Displacement r [pixels]', fontsize=12)\n",
    "ax1.set_ylabel('Structure Function', fontsize=12)\n",
    "ax1.set_title('All Structure Functions', fontsize=14)\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Elsasser variable asymmetry\n",
    "ax2.semilogx(r_centers[valid], dzp_mean[valid] / dzm_mean[valid], 'o-', color='black', markersize=8)\n",
    "ax2.axhline(1, linestyle='--', color='gray', alpha=0.5)\n",
    "ax2.set_xlabel('Displacement r [pixels]', fontsize=12)\n",
    "ax2.set_ylabel('|δz⁺| / |δz⁻|', fontsize=12)\n",
    "ax2.set_title('Elsasser Variable Asymmetry', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.5, 2.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Parameter Exploration\n",
    "\n",
    "If ipywidgets is available, let's create an interactive tool to explore how parameters affect the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_WIDGETS:\n",
    "    def interactive_sf_analysis(stride=1, n_samples=100, max_disp_fraction=0.25):\n",
    "        \"\"\"Interactive structure function analysis with adjustable parameters.\"\"\"\n",
    "        \n",
    "        # Reload data with new stride\n",
    "        slice_data_int = load_slice_npz(slice_file, stride=stride)\n",
    "        rho_int = slice_data_int[\"rho\"]\n",
    "        v_x_int = slice_data_int[\"v_x\"]\n",
    "        v_y_int = slice_data_int[\"v_y\"]\n",
    "        v_z_int = slice_data_int[\"v_z\"]\n",
    "        B_x_int = slice_data_int[\"B_x\"]\n",
    "        B_y_int = slice_data_int[\"B_y\"]\n",
    "        B_z_int = slice_data_int[\"B_z\"]\n",
    "        \n",
    "        # Set up displacements\n",
    "        N_res_int = rho_int.shape[0]\n",
    "        ell_max_int = int(N_res_int * max_disp_fraction)\n",
    "        \n",
    "        # Simple displacement selection for interactive use\n",
    "        displacements_int = [(i, 0) for i in range(1, ell_max_int, max(1, ell_max_int//10))]\n",
    "        \n",
    "        # Compute structure functions\n",
    "        r_vals = []\n",
    "        dv_vals = []\n",
    "        dB_vals = []\n",
    "        \n",
    "        for dx, dy in displacements_int:\n",
    "            dv, dB, drho, _, _ = compute_structure_functions_simple(\n",
    "                v_x_int, v_y_int, v_z_int, B_x_int, B_y_int, B_z_int, rho_int,\n",
    "                dx, dy, n_samples\n",
    "            )\n",
    "            r_vals.append(np.sqrt(dx**2 + dy**2))\n",
    "            dv_vals.append(np.mean(dv))\n",
    "            dB_vals.append(np.mean(dB))\n",
    "        \n",
    "        # Plot results\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Structure functions\n",
    "        ax1.loglog(r_vals, dv_vals, 'o-', label='Velocity', color='blue', markersize=8)\n",
    "        ax1.loglog(r_vals, dB_vals, 's-', label='Magnetic', color='red', markersize=8)\n",
    "        ax1.set_xlabel('Displacement r')\n",
    "        ax1.set_ylabel('Structure Function')\n",
    "        ax1.set_title(f'Structure Functions (stride={stride}, samples={n_samples})')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Field visualization\n",
    "        v_mag_int = np.sqrt(v_x_int**2 + v_y_int**2 + v_z_int**2)\n",
    "        im = ax2.imshow(v_mag_int, origin='lower', cmap='viridis')\n",
    "        ax2.set_title(f'Velocity Magnitude (grid size: {N_res_int}x{N_res_int})')\n",
    "        plt.colorbar(im, ax=ax2, fraction=0.046)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Create interactive widgets\n",
    "    print(\"Interactive Structure Function Explorer\")\n",
    "    print(\"Adjust parameters to see their effect on the analysis:\\n\")\n",
    "    \n",
    "    interact = widgets.interactive(\n",
    "        interactive_sf_analysis,\n",
    "        stride=widgets.IntSlider(value=1, min=1, max=4, step=1, \n",
    "                                description='Stride:'),\n",
    "        n_samples=widgets.IntSlider(value=100, min=50, max=500, step=50, \n",
    "                                   description='Samples:'),\n",
    "        max_disp_fraction=widgets.FloatSlider(value=0.25, min=0.1, max=0.5, step=0.05,\n",
    "                                             description='Max disp:')\n",
    "    )\n",
    "    display(interact)\n",
    "else:\n",
    "    print(\"Interactive features not available. Install ipywidgets for interactive exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Understanding the Full Pipeline\n",
    "\n",
    "The complete SFunctor pipeline computes 24 different structure function channels. Let's explore what these are and how they relate to MHD turbulence physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the 24 channels\n",
    "channel_info = [\n",
    "    (\"Basic Fields (9 channels):\", [\n",
    "        \"Channel 0-2: Velocity components (δvₓ, δvᵧ, δvᵤ)\",\n",
    "        \"Channel 3-5: Magnetic field components (δBₓ, δBᵧ, δBᵤ)\",\n",
    "        \"Channel 6-8: Density-weighted velocity (δ(ρv)ₓ, δ(ρv)ᵧ, δ(ρv)ᵤ)\"\n",
    "    ]),\n",
    "    (\"Elsasser Variables (6 channels):\", [\n",
    "        \"Channel 9-11: z⁺ = v + vₐ components\",\n",
    "        \"Channel 12-14: z⁻ = v - vₐ components\"\n",
    "    ]),\n",
    "    (\"Derived Quantities (9 channels):\", [\n",
    "        \"Channel 15-17: Vorticity ω = ∇ × v\",\n",
    "        \"Channel 18-20: Current density J = ∇ × B\",\n",
    "        \"Channel 21-23: Cross products and other correlations\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"24 Structure Function Channels in Full Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category, channels in channel_info:\n",
    "    print(f\"\\n{category}\")\n",
    "    for channel in channels:\n",
    "        print(f\"  • {channel}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nThese channels capture different aspects of MHD turbulence:\")\n",
    "print(\"- Basic fields reveal energy cascade in velocity and magnetic fields\")\n",
    "print(\"- Elsasser variables separate forward/backward Alfvén wave packets\")\n",
    "print(\"- Vorticity and current density probe small-scale structures\")\n",
    "print(\"- Cross correlations reveal coupling between fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison: Simple vs Full Analysis\n",
    "\n",
    "Let's compare the simplified analysis we just performed with what the full pipeline computes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    \"Aspect\": [\n",
    "        \"Parallelization\",\n",
    "        \"Performance\",\n",
    "        \"Channels computed\",\n",
    "        \"Displacement sampling\",\n",
    "        \"Stencil options\",\n",
    "        \"Memory usage\",\n",
    "        \"Output format\",\n",
    "        \"Dependencies\"\n",
    "    ],\n",
    "    \"Simple Analysis (this notebook)\": [\n",
    "        \"None (serial execution)\",\n",
    "        \"Slower (no Numba JIT)\",\n",
    "        \"3 (|δv|, |δB|, |δρ|)\",\n",
    "        \"Random subset\",\n",
    "        \"2-point only\",\n",
    "        \"Low\",\n",
    "        \"Simple arrays\",\n",
    "        \"NumPy only\"\n",
    "    ],\n",
    "    \"Full Pipeline (run_sf.py)\": [\n",
    "        \"MPI + shared memory\",\n",
    "        \"Fast (Numba-optimized)\",\n",
    "        \"24 (all components)\",\n",
    "        \"Systematic coverage\",\n",
    "        \"2, 3, or 5-point\",\n",
    "        \"Higher\",\n",
    "        \"Structured histograms\",\n",
    "        \"NumPy, Numba, MPI4Py\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\nComparison: Simple vs Full Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Aspect':<25} {'Simple (Demo)':<30} {'Full Pipeline':<25}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(len(comparison_data[\"Aspect\"])):\n",
    "    aspect = comparison_data[\"Aspect\"][i]\n",
    "    simple = comparison_data[\"Simple Analysis (this notebook)\"][i]\n",
    "    full = comparison_data[\"Full Pipeline (run_sf.py)\"][i]\n",
    "    print(f\"{aspect:<25} {simple:<30} {full:<25}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nWhen to use each approach:\")\n",
    "print(\"- Simple: Quick exploration, testing, understanding the method\")\n",
    "print(\"- Full: Production runs, large datasets, complete statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Analysis Results\n",
    "\n",
    "Let's save our analysis results in a format compatible with the visualization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the computed structure functions\n",
    "output_file = Path(\"demo_sf_results.npz\")\n",
    "\n",
    "np.savez(\n",
    "    output_file,\n",
    "    # Basic structure function data\n",
    "    displacements=all_displacements,\n",
    "    dv_values=all_dv_values,\n",
    "    dB_values=all_dB_values,\n",
    "    drho_values=all_drho_values,\n",
    "    \n",
    "    # Component data\n",
    "    dv_components=all_dv_components,\n",
    "    dB_components=all_dB_components,\n",
    "    \n",
    "    # Elsasser variables\n",
    "    dzp_values=all_dzp_values,\n",
    "    dzm_values=all_dzm_values,\n",
    "    \n",
    "    # Binned statistics\n",
    "    r_bins=r_bins,\n",
    "    r_centers=r_centers,\n",
    "    dv_mean=dv_mean,\n",
    "    dB_mean=dB_mean,\n",
    "    drho_mean=drho_mean,\n",
    "    \n",
    "    # Metadata\n",
    "    meta=dict(\n",
    "        slice=str(slice_file),\n",
    "        axis=axis,\n",
    "        beta=beta,\n",
    "        n_samples=N_random_subsamples,\n",
    "        n_displacements=n_disp_demo,\n",
    "        date=datetime.utcnow().isoformat(),\n",
    "        analysis_type='demo_notebook'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print(f\"File size: {output_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "### What We've Covered\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "\n",
    "1. **Data Loading**: How to load and explore 2D slices from MHD simulations\n",
    "2. **Field Visualization**: Creating comprehensive views of velocity, magnetic field, and density\n",
    "3. **Structure Function Analysis**: Computing increments at different displacements\n",
    "4. **Statistical Analysis**: Binning data and computing scaling behaviors\n",
    "5. **Comprehensive Plotting**: Various ways to visualize structure functions\n",
    "6. **Physics Insights**: Understanding Elsasser variables and MHD turbulence\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To use SFunctor for your own analysis:\n",
    "\n",
    "1. **Extract slices from your data**:\n",
    "   ```bash\n",
    "   python extract_2d_slice.py --input_file your_3d_data.bin --output_file slice.npz\n",
    "   ```\n",
    "\n",
    "2. **Run the full pipeline** (with MPI for large datasets):\n",
    "   ```bash\n",
    "   mpirun -n 64 python run_sf.py --file_name slice.npz --stride 2\n",
    "   ```\n",
    "\n",
    "3. **Visualize results**:\n",
    "   ```bash\n",
    "   python visualize_sf_results.py results_sf.npz\n",
    "   ```\n",
    "\n",
    "### Key Physics Insights\n",
    "\n",
    "- **Scaling laws**: Different fields show different scaling behaviors\n",
    "  - Velocity: Often close to Kolmogorov (1/3) or steeper\n",
    "  - Magnetic: Can show Iroshnikov-Kraichnan (1/2) scaling\n",
    "  - Density: Often shows shallower scaling\n",
    "\n",
    "- **Elsasser asymmetry**: The ratio |δz⁺|/|δz⁻| reveals the balance between forward and backward Alfvén waves\n",
    "\n",
    "- **Intermittency**: Heavy tails in increment PDFs indicate intermittent structures\n",
    "\n",
    "### References\n",
    "\n",
    "For more on MHD turbulence and structure functions, see:\n",
    "- Biskamp, D. (2003). Magnetohydrodynamic Turbulence\n",
    "- Boldyrev, S. (2006). Spectrum of Magnetohydrodynamic Turbulence\n",
    "- Beresnyak, A. & Lazarian, A. (2019). MHD Turbulence, Turbulent Dynamo and Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Demo notebook complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAnalyzed slice: {slice_file.name if 'slice_file' in locals() else 'No slice loaded'}\")\n",
    "print(f\"Results saved to: {output_file if 'output_file' in locals() else 'No results saved'}\")\n",
    "print(\"\\nThank you for using SFunctor!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
